import torch
import torch.nn as nn
import torch.optim as optim
from transformers import BertModel, GPT2Model, T5Model

class EMMAT:
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self.build_model().to(self.device)
        self.optimizer = optim.AdamW(self.model.parameters(), lr=config.learning_rate)
        self.criterion = nn.CrossEntropyLoss()

    def build_model(self):
        # Define the modality-specific encoders
        text_encoder = BertModel.from_pretrained('bert-base-uncased')
        image_encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(128 * 7 * 7, 512),
            nn.ReLU()
        )
        audio_encoder = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Flatten(),
            nn.Linear(128 * 30, 512),
            nn.ReLU()
        )

        # Define the shared transformer and task-specific adapters
        shared_transformer = GPT2Model.from_pretrained('gpt2')
        task_adapters = nn.ModuleDict({
            'classification': nn.Sequential(
                nn.Linear(768, 512),
                nn.ReLU(),
                nn.Linear(512, config.num_classes)
            ),
            'generation': nn.Sequential(
                nn.Linear(768, 512),
                nn.ReLU(),
                nn.Linear(512, config.vocab_size)
            )
        })

        return nn.ModuleDict({
            'text_encoder': text_encoder,
            'image_encoder': image_encoder,
            'audio_encoder': audio_encoder,
            'shared_transformer': shared_transformer,
            'task_adapters': task_adapters
        })

    def train(self, train_dataloader, val_dataloader, num_epochs):
        for epoch in range(num_epochs):
            self.model.train()
            for batch in train_dataloader:
                # Multi-modal pre-training
                text_inputs, image_inputs, audio_inputs, labels = batch
                text_inputs, image_inputs, audio_inputs, labels = (
                    text_inputs.to(self.device),
                    image_inputs.to(self.device),
                    audio_inputs.to(self.device),
                    {task: labels[task].to(self.device) for task in labels}
                )
                text_embeddings = self.model['text_encoder'](text_inputs)[0]
                image_embeddings = self.model['image_encoder'](image_inputs)
                audio_embeddings = self.model['audio_encoder'](audio_inputs)
                multi_modal_embeddings = torch.cat((text_embeddings, image_embeddings, audio_embeddings), dim=1)

                # Shared transformer encoding
                transformer_outputs = self.model['shared_transformer'](multi_modal_embeddings)[0]

                # Task-specific adaptation
                task_outputs = {}
                for task, adapter in self.model['task_adapters'].items():
                    task_outputs[task] = adapter(transformer_outputs)

                # Compute losses and backpropagate
                loss = 0
                for task, output in task_outputs.items():
                    task_loss = self.criterion(output, labels[task])
                    loss += task_loss

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

            # Evaluate on validation set
            val_loss, val_metrics = self.evaluate(val_dataloader)
            print(f"Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_loss:.4f}, Val Metrics: {val_metrics}")

    def evaluate(self, dataloader):
        self.model.eval()
        total_loss = 0
        total_samples = 0
        task_metrics = {}

        with torch.no_grad():
            for batch in dataloader:
                text_inputs, image_inputs, audio_inputs, labels = batch
                text_inputs, image_inputs, audio_inputs, labels = (
                    text_inputs.to(self.device),
                    image_inputs.to(self.device),
                    audio_inputs.to(self.device),
                    {task: labels[task].to(self.device) for task in labels}
                )
                text_embeddings = self.model['text_encoder'](text_inputs)[0]
                image_embeddings = self.model['image_encoder'](image_inputs)
                audio_embeddings = self.model['audio_encoder'](audio_inputs)
                multi_modal_embeddings = torch.cat((text_embeddings, image_embeddings, audio_embeddings), dim=1)

                transformer_outputs = self.model['shared_transformer'](multi_modal_embeddings)[0]

                task_outputs = {}
                for task, adapter in self.model['task_adapters'].items():
                    task_outputs[task] = adapter(transformer_outputs)

                loss = 0
                for task, output in task_outputs.items():
                    task_loss = self.criterion(output, labels[task])
                    loss += task_loss

                    # Compute task-specific metrics
                    preds = output.argmax(dim=1)
                    accuracy = (preds == labels[task]).float().mean()
                    if task not in task_metrics:
                        task_metrics[task] = {'accuracy': []}
                    task_metrics[task]['accuracy'].append(accuracy.item())

                total_loss += loss.item() * text_inputs.size(0)
                total_samples += text_inputs.size(0)

        avg_loss = total_loss / total_samples
        avg_metrics = {task: {metric: sum(values) / len(values) for metric, values in metrics.items()} for task, metrics in task_metrics.items()}

        return avg_loss, avg_metrics

    def prune_and_distill(self, dataloader, pruning_threshold):
        # Dynamic knowledge distillation
        student_model = self.build_model().to(self.device)
        student_optimizer = optim.AdamW(student_model.parameters(), lr=self.config.learning_rate)

        for epoch in range(self.config.distillation_epochs):
            student_model.train()
            for batch in dataloader:
                text_inputs, image_inputs, audio_inputs, labels = batch
                text_inputs, image_inputs, audio_inputs, labels = (
                    text_inputs.to(self.device),
                    image_inputs.to(self.device),
                    audio_inputs.to(self.device),
                    {task: labels[task].to(self.device) for task in labels}
                )
                with torch.no_grad():
                    teacher_outputs = self.model(text_inputs, image_inputs, audio_inputs)
                student_outputs = student_model(text_inputs, image_inputs, audio_inputs)

                distillation_loss = 0
                for task in self.model['task_adapters']:
                    distillation_loss += nn.KLDivLoss()(nn.LogSoftmax(dim=1)(student_outputs[task]), nn.Softmax(dim=1)(teacher_outputs[task]))

                student_optimizer.zero_grad()
                distillation_loss.backward()
                student_optimizer.step()

        # Pruning
        for name, param in self.model.named_parameters():
            if 'task_adapters' not in name:
                mask = torch.abs(param) < pruning_threshold
                param.data[mask] = 0

        self.model = student_model

    def save_model(self, path):
        torch.save(self.model.state_dict(), path)

    def load_model(self, path):
        self.model.load_state_dict(torch.load(path))
